{
    "componentChunkName": "component---src-templates-blog-post-js",
    "path": "/simple-SUPERTRACK/",
    "result": {"data":{"site":{"siteMetadata":{"title":"Geunsik Jo's resume"}},"markdownRemark":{"id":"0518c752-e190-573b-bf48-d451b12cf0df","excerpt":"SuperTrack: Motion Tracking for Physically Simulated Characters using\r\nSupervised Learning 소개 현재 신경망을 사용하는 로봇제어 방법론에는 PPO, TRPO…","html":"<p><a href=\"https://theorangeduck.com/media/uploads/other_stuff/SuperTrack.pdf\">SuperTrack: Motion Tracking for Physically Simulated Characters using\r\nSupervised Learning</a></p>\n<h3>소개</h3>\n<p>현재 신경망을 사용하는 로봇제어 방법론에는 PPO, TRPO 등과 같은 강화학습을 사용하는 알고리즘이 사용되고 있습니다.</p>\n<p>그러나 위와 같은 강화학습 방법론은 긴 학습시간, 보상, 물리엔진설정, 시드값등 매우 민감하게 반응합니다.</p>\n<p>SuperTrack은 이런 문제점을 해결하기 위해 지도학습 방법론을 사용하여 안정적인 로봇 동작 학습을 실현하였습니다.</p>\n<h3>모델</h3>\n<p>SuperTrack은 다양한 물리적 상태를 입력으로 받아 원하는 자세 제어값을 출력합니다.</p>\n<p>이러한 작업에는 일반적으로 강화학습을 사용하게 됩니다.</p>\n<p>그러나 물리엔진의 복잡한 여러가지 연산은 역전파학습을 수행해야하는 딥러닝 알고리즘이 제한됩니다.</p>\n<p>SuperTrack은 이러한 물리엔진의 물리엔진을 모사하는 신경망네트워크를 병렬로 두고 학습하여 해결합니다.</p>\n<p><img src=\"/aef2b181d2774b984ecf73fde0a46086/CCC.png\" alt=\"Chinese Salty Egg\">\r\n위 그림과 같이 SuperTrack은 Policy Network와 World Network로 구성되어 있습니다.</p>\n<p>여기서 Policy Network는 로봇의 중심을 기준으로하는 로컬 좌표계의 [각도, 각가속도, 위치, 가속도, 바닥에서 머리까지 높이, 직립도]를 입력으로 받아 제어 대상의 모터 PD 제어의 [offset 값]을 출력합니다.</p>\n<p>World Network는 offset값과 State를 입력으로 받아서 State를 출력합니다.</p>\n<h2>Network Structure</h2>\n<p><img src=\"/387af9f32b8380d1fad3db26a7db4d91/EE.png\" alt=\"Chinese Salty Egg\">\r\n(좌) PolicyNetwork (우) WorldNetwork</p>\n<p>좌측 PolicyNetwork는 Local State와 Target Pose를 입력으로 받으며 2레이어 128개의 가중치로 구성되어 있습니다.<br>\nControl offset을 출력합니다.</p>\n<p>우측 WorldNetwork는 Local State와 Control Offset을 입력으로 받으며 2레이어 128개의 가중치로 구성되어 있습니다.<br>\nLocal State를 출력합니다.</p>\n<p>두 네트워크는 활성함수로 Elu를 사용합니다.</p>\n<h2>Training Algorithm</h2>\n<p>학습은 세가지 단계로 이루어집니다.</p>\n<p>첫번째는 학습데이터를 모으는 과정입니다.</p>\n<p><img src=\"/19483cb8bd0dab4205d9eded3ca444c6/A.png\" alt=\"Chinese Salty Egg\"></p>\n<blockquote>\n<ol>\n<li>먼저 물리엔진에 상태값을 받아옵니다. 이 값을 S라고 정의합니다.</li>\n<li>그 후 로컬좌표계로 변환하게 됩니다.</li>\n<li>로컬좌표계로 변환된 값은 1차원 배열로 바꿉니다.</li>\n<li>1차원된 배열은 Policy 네트워크로 들어가게 됩니다.</li>\n<li>그 후 탐색범위를 넓히기 위해 Normal noise를 첨가하게 됩니다. 이 값은 T 라고 정의합니다.</li>\n<li>값 T는 물리엔진에 들어가게 됩니다.</li>\n<li>순환 버퍼에 S, K, T를 넣습니다. 여기서 K는 목표로하는 물리엔진 케릭터의 자세입니다.</li>\n<li>물리엔진은 값 T를 거쳐 나온 값을 다시 S에 넣습니다.</li>\n</ol>\n</blockquote>\n<p>두번째는 WorldModel 학습을 시작합니다.</p>\n<p><img src=\"/7ee8c2af0a0789da388cd660037f2c3c/B.png\" alt=\"Chinese Salty Egg\"></p>\n<blockquote>\n<ol>\n<li>버퍼에서 랜덤하게 S, K, T 청크를 뽑습니다. 주의할 점은 이 경우에 청크의 크기는 WorldModel 스텝 크기의 배수여야 합니다.</li>\n<li>S의 첫번째 값을 뽑아 P[0]에 넣습니다.</li>\n<li>S값을 Local 좌표계로 전환하고 WorldModel에 T 값과 넣게 됩니다.</li>\n<li>WorldModel을 거쳐서 나온 값은 가가속도와 가각속도 입니다.</li>\n<li>이 값을 월드 좌표계로 바꿉니다.</li>\n<li>이를 P[N]값과 Integrate 과정을 거쳐 새로운 P[N+1]값을 만들어 냅니다.</li>\n<li>정의된 Loss 함수를 사용해 손실 값을 구한뒤 Backpropagation 합니다.</li>\n</ol>\n</blockquote>\n<p>세번째 PolicyModel 학습을 시작합니다.</p>\n<p><img src=\"/32b4b905b4f812b161cfd66c89e5df81/C.png\" alt=\"Chinese Salty Egg\"></p>\n<blockquote>\n<ol>\n<li>버퍼에서 랜덤하게 S, K, T 청크를 뽑습니다. 주의할 점은 마찬가지로 청크의 크기는 PolicyModel 스텝 크기의 배수여야 합니다.</li>\n<li>S의 첫번째 값을 뽑아 P[0]에 넣습니다.</li>\n<li>P[N]값과 K[N]를 Local 좌표계로 전환하고 PolicyModel 넣게 됩니다.</li>\n<li>첫번째 데이터를 모으는 과정처럼 출력값에는 noise를 첨가하여 탐색범위를 넓힙니다.</li>\n<li>출력된 값을 이 전 상태값과 같이 WorldModel에 넣습니다.</li>\n<li>이를 _S 값과 Integrate 과정을 거쳐 새로운 _S 값을 만들어 냅니다.</li>\n<li>_S를 새로운 배열 버퍼에 저장합니다.</li>\n<li>정의된 Loss 함수를 사용해 손실 값을 구한뒤 Backpropagation 합니다.</li>\n</ol>\n</blockquote>\n<p>설명한 단계를 차례대로 혹은 비 순차적으로 반복하여 학습을 진행하게 됩니다.</p>\n<h2>Model Input Pipeline</h2>\n<p>물리엔진에서 출력되는 값은 통상적으로</p>\n<blockquote>\n<ul>\n<li>위치값 => 실수 [X, Y, Z]</li>\n<li>가속도 => 실수 [X, Y, Z]</li>\n<li>회전각 => 실수 쿼터니언 [W, X, Y, Z]</li>\n<li>각속도 => 실수 [X, Y, Z]</li>\n</ul>\n</blockquote>\n<p>위와 같습니다.</p>\n<p>딥 러닝 모델은 벡터 시계열 형태의 인풋을 받으므로, 모델을 학습시키기 위해서는 세가지 아래와 같은 변환을 거쳐야 합니다.</p>\n<p>1 . (Hip Transformation:골반을 중심 기준으로 삼음)강체 좌표를 Local 좌표계로 변환합니다.</p>\n<p>2 . 다음으로 쿼터니언을 신경망 기반 방법에서 일반적으로 사용되는 2축 회전 행렬 형식으로 변환합니다.<br>\n쿼터니언 -> 회전행렬\r\n[\r\n[1, 0, 0],\r\n[0, 1, 0 ],\r\n[0, 0, 1 ],\r\n]<br>\n회전행렬 -> 2축 회전 행렬 변환\r\n[\r\n[1, 0 ],\r\n[0, 1 ],\r\n[0, 0 ]\r\n]</p>\n<p>3 . 그런 다음 중심에 국한된 위쪽 방향뿐만 아니라 지면에서 모든 강체의 높이를 표현에 추가합니다.</p>\n<p>결과적으로 아래와 같은 형식을 갖추게 됩니다.</p>\n<blockquote>\n<ul>\n<li>위치값 => 실수 ([X, Y, Z] - [X_root, Y_root, Z_root]) @ inv_R_root</li>\n<li>가속도 => 실수 [X, Y, Z] @ inv_R_root</li>\n<li>회전각 => 실수 회전행렬 [[1, 0 ],[0, 1 ],[0, 0 ]] @ inv_R_root</li>\n<li>각속도 => 실수 [X, Y, Z] @ inv_R_root</li>\n<li>높이 => 실수 [Z ]</li>\n<li>직립도 => [1, 0, 0] @ inv_R_root</li>\n</ul>\n</blockquote>\n<p>해당 값들을 펼쳐서 1차원 배열로 만든 뒤 네트워크에 입력합니다.</p>\n<h2>Learning Time Step Limit</h2>\n<p>WorldModel의 학습은 로봇의 다음 포즈 예측을 기반으로 합니다.<br>\n복잡한 동작의 경우, 단순한 학습은 불안정 할 수 있으므로 짧은 프레임 학습을 통해 안정성을 높입니다.</p>\n<p><img src=\"/95d2b519e55138f1b9cd99864ec9368a/LIMIT.png\" alt=\"Chinese Salty Egg\"></p>\n<h2>L1, L2</h2>\n<p>L1 함수는 WorldModel과 PolicyModel 모두에 공통적으로 사용되며 L2 함수는 PolicyModel에만 사용됩니다.<br>\nL2 함수는 출력값의 폭발을 방지하여 학습의 안정성을 높입니다.</p>\n<h2>물리엔진의 선정</h2>\n<p>물리엔진은 PHYSIX, BULLET, HAVOK 등이 고려되었으나, MUJOCO가 라이센스 문제 없이 사용이 가능하고 성능도 우수하며 Python과 쉽게 통합 가능하기에 선택되었습니다.</p>\n<p>필요에따라 Nvidia의 아이작심이나 카이스트 라이랩의 라이심을 사용해 볼 수도 있습니다.</p>\n<h2>구현</h2>\n<p>코드는 아래의 링크에서 확인할 수 있습니다.</p>\n<p><a href=\"https://github.com/kistf001/simple_superTrack\">https://github.com/kistf001/simple_superTrack</a></p>","frontmatter":{"title":"Simple SuperTrack","date":"May 01, 2021","description":"Simple SuperTrack은 물리시뮬레이션에서 캐릭터 컨트롤러를 학습하기 위한 지도학습방법을 제안한 supertrack을 구현체 입니다."}},"previous":{"fields":{"slug":"/JADAL/"},"frontmatter":{"title":"JADAL"}},"next":{"fields":{"slug":"/introduction/"},"frontmatter":{"title":"소개"}}},"pageContext":{"id":"0518c752-e190-573b-bf48-d451b12cf0df","previousPostId":"1e671910-2b50-5a8d-9744-18ccfd11a6e8","nextPostId":"608a256e-d558-50d7-810e-176ed168ce29"}},
    "staticQueryHashes": ["2841359383","3257411868"]}